[toc]

## 部署 cin 网络插件

**为什么要部署 cin 网络插件**

* 使不同宿主机的 pod 能够网络互通

**常见的 cin 网络插件**：

* flannel - `当前使用`
* calico
* canal
* contiv
* OpenContrail
* NSX-T
* Kube-router

### 下载 flannel 二进制包

[releases](https://github.com/coreos/flannel/releases/tag/v0.13.1-rc2)

```sh
cd /opt/packages/
wget https://github.com/coreos/flannel/releases/download/v0.13.1-rc2/flannel-v0.13.1-rc2-linux-amd64.tar.gz
mkdir flannel-v0.13.1-rc2
tar -xf flannel-v0.13.1-rc2-linux-amd64.tar.gz -C flannel-v0.13.1-rc2/
mkdir -p /opt/flannel/certs
cd /opt/flannel
ln -snf /opt/packages/flannel-v0.13.1-rc2/flanneld flanneld
```

### 拷贝证书

```sh
cd /opt/certs/k8s
scp ca.pem etcd-client.pem etcd-client-key.pem lc103-200:/opt/flannel/certs
```

### 创建配置

```sh
cd /opt/flannel
vim subnet.env
```

```env
FLANNEL_NETWORK=172.7.0.0/16   # pod的网段
FLANNEL_SUBNET=172.7.200.1/24  # 本机运行pod的网段
FLANNEL_MTU=1500
FLANNEL_IPMASQ=false
```

> 注意这里的网段设置为我们规划的网段

### 创建启动脚本

```sh
vim flanneld-startup.sh
```

```sh
#!/bin/sh
./flanneld \
  --public-ip=172.16.103.200 \
  --etcd-endpoints=https://172.16.103.200:23791,https://172.16.103.200:23792,https://172.16.103.200:23793 \
  --etcd-keyfile=./certs/etcd-client-key.pem \
  --etcd-certfile=./certs/etcd-client.pem \
  --etcd-cafile=./certs/ca.pem \
  --iface=ens33 \
  --subnet-file=./subnet.env \
  --healthz-port=2401
```

> * `--public-ip` - 本机ip
> * `--iface` - 绑定网卡

```sh
chmod u+x flanneld-startup.sh
```

### 在etcd中创建配置

```sh
cd /opt/etcd/
./etcdctl --endpoints "http://127.0.0.1:23791" set /coreos.com/network/config '{"Network": "172.7.0.0/16", "Backend": {"Type": "host-gw"}}'
```

> 注意修改这里的 ``"Network"`` 和 `subnet.env` 中的保持一致

### 使用supervisord启动flannel

```sh
vim /etc/supervisord.d/flannel.ini 
```

```ini
[program:flanneld-alice002]
command=/opt/flannel/flanneld-startup.sh                     ; 
numprocs=1                                                   ; 
directory=/opt/flannel                                       ; 
autostart=true                                               ; 
autorestart=true                                             ; 
startsecs=30                                                 ; 
startretries=3                                               ; 
exitcodes=0,2                                                ; 
stopsignal=INT                                               ; 
stopwaitsecs=10                                              ; 
user=root                                                    ; 
redirect_stderr=true                                         ; 
stdout_logfile=/data/logs/flanneld/flanneld.stdout.log       ; 
stdout_logfile_maxbytes=64MB                                 ; 
stdout_logfile_backups=7                                     ; 
stdout_capture_maxbytes=1MB                                  ; 
stdout_events_enabled=false                                  ; 
```

```sh
mkdir -p /data/logs/flanneld/
supervisorctl update
supervisorctl status
tail -f /data/logs/flanneld/flanneld.stdout.log

route -n
```



### 测试

> 需要两个运算节点才能测试



flannel host-gw模型的原理就是：给宿主机添加一个静态路由，经由宿主机到达pod

Flannel的host-gw模型，所有node ip必须在同一个物理网管设备下才能使用

注意：阿里云不支持iptables的nat表，需要用到VxLAN模型

### SNAT规则优化

```sh
# iptables-save | grep -i postrouting
:POSTROUTING ACCEPT [4307608:818225311]
:POSTROUTING ACCEPT [17:1028]
:KUBE-POSTROUTING - [0:0]
-A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING
-A POSTROUTING -s 172.7.103.0/24 ! -o docker0 -j MASQUERADE
-A POSTROUTING -s 172.17.0.0/16 ! -o br-5ea8a33e5c25 -j MASQUERADE
-A POSTROUTING -s 172.17.0.4/32 -d 172.17.0.4/32 -p tcp -m tcp --dport 10514 -j MASQUERADE
-A POSTROUTING -s 172.17.0.9/32 -d 172.17.0.9/32 -p tcp -m tcp --dport 8080 -j MASQUERADE
-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN
-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -j MASQUERADE
```

这里我们看到这一条：

```
-A POSTROUTING -s 172.7.103.0/24 ! -o docker0 -j MASQUERADE
```

意思是只要不是从 docker0这个设备出网且网段为 172.7.103.0/24 的流量，就会做一个宿主机的IP地址转换

我们需要做以下优化

```sh
yum install iptables-services -y
iptables -t nat -D POSTROUTING -s 172.7.200.0/24 ! -o docker0 -j MASQUERADE
iptables -t nat -I POSTROUTING -s 172.7.200.0/24 ! -d 172.7.0.0/16 ! -o docker0 -j MASQUERADE
```

## 部署 core-dns

### 配置资源配置清单访问入口

在运维主机 lc103-200.host.com上，配置一个nginx虚拟主机，用以提供K8S统一的资源配置清单访问入口

```sh
mkdir /data/k8s-yaml
vi /etc/nginx/conf.d/k8s-yaml.od.com.conf
```

```conf
server {
    listen       81;
    server_name  k8s-yaml.od.com;
    location / {
        autoindex on;
        default_type text/plain;
        root /data/k8s-yaml;
    }
}
```

```sh
nginx -t
nginx -s reload
```

### 配置内网 dns 解析

```sh
vi /var/named/od.com.zone
```

添加：

```
k8s-yaml           A    192.168.16.12		# 添加地址解析
```

```sh
systemctl restart named
dig -t A k8s-yaml.od.com @172.16.103.200  +short
```

### 拉取镜像

```sh
docker pull coredns/coredns:1.6.1
docker tag coredns/coredns:1.6.1 harbor.od.com/public/coredns:v1.6.1
docker push harbor.od.com/public/coredns:v1.6.1
```

### 创建资源配置清单

[参考地址](https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/coredns/coredns.yaml.base)

```
mkdir -p /data/k8s-yaml/coredns
cd /data/k8s-yaml/coredns
```

##### vim  rbac.yaml

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
  labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: Reconcile
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: EnsureExists
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
```

##### vim  cm.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
  labels:
      addonmanager.kubernetes.io/mode: EnsureExists
data:
  Corefile: |
    .:53 {
        errors
        health 
        ready
        kubernetes cluster.local 192.168.0.0/16
        forward . 172.16.103.200
        cache 30
        loop
        reload
        loadbalance
    }
```

>* `kubernetes` - 设置服务网段
>* `forward` - 指向上层dns

##### vim dp.taml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  # replicas: not specified here:
  # 1. In order to make Addon Manager do not reconcile this replicas parameter.
  # 2. Default is 1.
  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: k8s-app
                    operator: In
                    values: ["kube-dns"]
              topologyKey: kubernetes.io/hostname
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      nodeSelector:
        kubernetes.io/os: linux
      containers:
      - name: coredns
        image: harbor.od.com/public/coredns:v1.6.1
        imagePullPolicy: IfNotPresent
        resources:
          # limits:
          #  memory: __DNS__MEMORY__LIMIT__
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
```

##### vim service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 192.168.0.2
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  - name: metrics
    port: 9153
    protocol: TCP
```

> `clusterIP` - 设定为 kubelet 启动时参数中的 cluster-dns，这个应该在规划时设定好的

### 部署

```sh
kubectl apply -f http://k8s-yaml.od.com:81/coredns/rbac.yaml
kubectl apply -f http://k8s-yaml.od.com:81/coredns/cm.yaml
kubectl apply -f http://k8s-yaml.od.com:81/coredns/dp.yaml
kubectl apply -f http://k8s-yaml.od.com:81/coredns/service.yaml
```

### 验证

```sh
# kubectl get svc -n kube-system
NAME       TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   192.168.0.2   <none>        53/UDP,53/TCP,9153/TCP   19s

# kubectl get all -n kube-system
NAME                           READY   STATUS    RESTARTS   AGE
pod/coredns-74d497fd56-qnm4h   1/1     Running   0          64s

NAME               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns   ClusterIP   192.168.0.2   <none>        53/UDP,53/TCP,9153/TCP   56s

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   1/1     1            1           64s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-74d497fd56   1         1         1       64s

# dig -t A www.baidu.com @172.18.0.2 +short
www.a.shifen.com.
39.156.66.18
39.156.66.14

# dig -t A lc103-200.host.com @172.16.103.200 +short
172.16.103.200
```



```sh
# kubectl create deployment nginx-dp --image=harbor.od.com/public/nginx:v1.7.9 -n kube-public

# kubectl expose deployment nginx-dp --port=8081 -n kube-public

# kubectl get service -n kube-public
NAME       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
nginx-dp   ClusterIP   192.168.34.217   <none>        8081/TCP   8m14s

# 在集群外部只能用完整的域名进行获取
# dig -t A nginx-dp.kube-public.svc.cluster.local. @192.168.0.2 +short
192.168.34.217

# --------------------------------

# 在集群内部可以使用短域名进行获取
# kubectl exec -ti nginx-ds-pcnk9 /bin/bash
# /bin/ping nginx-dp.kube-public
PING nginx-dp.kube-public.svc.cluster.local (192.168.34.217): 48 data bytes
56 bytes from 192.168.34.217: icmp_seq=0 ttl=64 time=0.038 ms
56 bytes from 192.168.34.217: icmp_seq=1 ttl=64 time=0.063 ms

# 能使用短域名的原因是
# cat /etc/resolv.conf
nameserver 192.168.0.2
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
```

## 部署ingress插件 - 服务暴露 

1. K8S的DNS实现了服务在集群“内”被自动发现，那如何使得服务在k8s集群“外”被使用和访问呢
   * 使用NodePort型的Service
     * 注意：无法使用kube-proxy的ipvs模型
   * 使用Ingress资源
     * 注意：Ingress只能调度并暴露7层应用，特指http和https协议

2. Ingress是K8S API的标准资源类型之一，也是一种核心资源，它其实是一组基于域名和URL路径
   把用户的请求转发至指定Service资源的规则

3. 可以将集群外部的请求流量，转发至集群内部，从而实现“服务暴露”

4. Ingress控制器是能够为Ingress资源监听某些套接字，然后根据Ingress规则匹配机制路由调度流量的一组组件

5. 说白了，Ingress没啥神秘的，就是一个简化版的nginx+一段go脚本而已

6. 常用的Ingress控制器的实现软件
   * Ingress-nginx
   * HAProxy
   * Traefik

### 准备traefik镜像

```sh
docker pull traefik:v2.4.0
docker tag traefik:v2.4.0 harbor.od.com/public/traefik:v2.4.0
docker push harbor.od.com/public/traefik:v2.4.0

docker pull traefik:v1.7
```

### 创建资源配置清单

[参考清单](https://github.com/traefik/traefik/tree/v1.7/examples/k8s)

```sh
mkdir -p /data/k8s-yaml/traefik
cd /data/k8s-yaml/traefik
```

#### vim rbac.yaml

```yaml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: traefik-ingress-controller
rules:
  - apiGroups:
      - ""
    resources:
      - services
      - endpoints
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
      - networking.k8s.io   # 新增
    resources:
      - ingresses
      - ingressclasses      # 新增
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - ingresses/status
    verbs:
      - update
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: traefik-ingress-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik-ingress-controller
subjects:
- kind: ServiceAccount
  name: traefik-ingress-controller
  namespace: kube-system
```

#### vim cm.yaml

```yaml
# 部署 traefik 2.x 时 需要在 ds 或 dp 中进行相关配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: traefik-config
  namespace: kube-system
data:
  traefik.toml: |
    defaultEntryPoints = ["http","https"]
    debug = false
    logLevel = "INFO"
    # Do not verify backend certificates (use https backends)
    InsecureSkipVerify = true
    [entryPoints]
      [entryPoints.http]
      address = ":80"
      compress = true
      [entryPoints.https]
      address = ":443"
        [entryPoints.https.tls]
    #Config to redirect http to https
    #[entryPoints]
    #  [entryPoints.http]
    #  address = ":80"
    #  compress = true
    #    [entryPoints.http.redirect]
    #    entryPoint = "https"
    #  [entryPoints.https]
    #  address = ":443"
    #    [entryPoints.https.tls]
    [web]
      address = ":8080"
    [kubernetes]
    [metrics]
      [metrics.prometheus]
      buckets=[0.1,0.3,1.2,5.0]
      entryPoint = "traefik"
    [ping]
    entryPoint = "http"
```



#### vim dp.yaml

```yaml
# 先不要使用这个
# 这个和 ds.yaml 选择一个进行部署
# Deployment 自行确定 pod 个数
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
  labels:
    k8s-app: traefik-ingress-lb
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: traefik-ingress-lb
  template:
    metadata:
      labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
      serviceAccountName: traefik-ingress-controller
      terminationGracePeriodSeconds: 60
      containers:
      - image: harbor.od.com/public/traefik:v1.7
        name: traefik-ingress-lb
        ports:
        - name: controller
          containerPort: 80
        - name: admin-web
          containerPort: 8080
        args:
        - --api
        - --logLevel=INFO
        - --insecureskipverify=true
        - --kubernetes.endpoint=https://172.16.103.20:7443              # keepalive VIP的地址
        - --accesslog
        - --accesslog.filepath=/var/log/traefik_access.log
        - --traefiklog
        - --traefiklog.filepath=/var/log/traefik.log
        - --metrics.prometheus
      volumes:    # 部署 traefik 2.x 时需要
      - configMap:
          name: traefik-config
        name: config
```

#### vim ds.yaml

```yaml
# 这个和 dp.yaml 选择一个进行部署
# DaemonSet 在每台机器上启动一个实例，节点数量在 3 - 5 个时用这个比较合适，也许参考业务
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
---
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
  labels:
    k8s-app: traefik-ingress-lb
spec:
  selector:
    matchLabels:
      k8s-app: traefik-ingress-lb
      name: traefik-ingress-lb
  template:
    metadata:
      labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
      serviceAccountName: traefik-ingress-controller
      terminationGracePeriodSeconds: 60
      containers:
      - image: harbor.od.com/public/traefik:v2.4.0
        name: traefik-ingress-lb-v2
        ports:
        - name: web
          containerPort: 80
          hostPort: 90
        - name: admin
          containerPort: 8080
          hostPort: 8090
        - name: websecure
          containerPort: 443
        securityContext:
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
        args:
        - --api
        - --api.insecure=true
        - --providers.kubernetesingress=true
        - --log.level=INFO
        #- --providers.kubernetescrd # use this when using IngressRoute
        - --providers.kubernetesingress # use this when using Ingress
        - --entryPoints.web.address=:80
        # - --entrypoints.web.http.redirections.entryPoint.to=websecure
        # - --entrypoints.web.http.redirections.entryPoint.scheme=https
        - --entryPoints.websecure.address=:443
        - --accesslog
        - --accesslog.filepath=/var/log/traefik_access.log
        - --traefiklog
        - --traefiklog.filepath=/var/log/traefik.log
        # - --certificatesResolvers.letsencrypt.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory
        # - --certificatesResolvers.letsencrypt.acme.tlsChallenge
        # - --certificatesresolvers.letsencrypt.acme.email=myemail@gmail.com
        # - --certificatesResolvers.letsencrypt.acme.storage=/data/acme.json

      volumes:    # 部署 traefik 2.x 时需要
      - configMap:
          name: traefik-config
        name: config
```

#### vim service.yaml

```yaml
kind: Service
apiVersion: v1
metadata:
  name: traefik-ingress-service
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
    - protocol: TCP
      port: 80
      name: web
    - protocol: TCP
      port: 8080
      name: admin
```

#### vim ingress.yaml

```yaml
---
#apiVersion: extensions/v1beta1
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: traefik-web-ui
  namespace: kube-system
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: websecure
    traefik.ingress.kubernetes.io/router.tls.certresolver: letsencrypt
    traefik.ingress.kubernetes.io/router.tls.domains.0.main: traefik.example.in
spec:
  rules:
  - host: traefik.od.com
    http:
      paths:
      - path: /
        backend:
          serviceName: traefik-web-ui
          servicePort: web
```

#### vim ui-service.yaml

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: traefik-web-ui
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
  - name: web
    port: 80
    targetPort: 8080
```

### 提交资源清单

```sh
kubectl apply -f http://k8s-yaml.od.com:81/traefik/rbac.yaml
kubectl apply -f http://k8s-yaml.od.com:81/traefik/cm.yaml
kubectl apply -f http://k8s-yaml.od.com:81/traefik/dp.yaml
kubectl apply -f http://k8s-yaml.od.com:81/traefik/ui.yaml
```

vi /etc/nginx/conf.d/od.com.conf 

```

upstream default_backend_traefik {
    server 172.16.103.200:801   max_fails=3 fail_timeout=10s; # 此ip为node ip+81端口，每个node节点都需要加上
}
server {
    listen      82;
    server_name *.od.com;					# 泛域名匹配，凡是 od.com 的域名内的 http 服务。都给到 ingress 里面
    location / {
        proxy_pass http://default_backend_traefik;
        proxy_set_header Host            $http_host;
        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;
    }
}
```



