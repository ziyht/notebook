最近线上使用InfluxDB时遇到当进行某个大查询后，内存始终保持在很高的水位，并且改内存水位一直保持很高的水平，没有下降的痕迹，排查后发现线上的InfluxDB是使用golang 1.13进行编译的，并且线上的centos系统使用的是linux 4.9的内核，linux系统内核在4.5版本之后就支持MADV_FREE操作，golang在1.12版本以后如果内核支持MADV_FREE就使用MADV_FREE来释放内存，但是MADV_FREE是lazy free，只有当系统内存使用有压力的时候才会进行真正的内存回收，这个时候RSS才会下降，集体golang 1.12内存回收释放可以参考Go 1.12 关于内存释放的一个改进，知道原因后通过GODEBUG=madvdontneed=1 ./influxdb的方式重新启动，内存水位恢复正常
————————————————
版权声明：本文为CSDN博主「孤寂的夜总有繁星点点」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/u012794915/java/article/details/105450680





# 关于influxdb（1.x）使用的一些总结（包含一些v2使用的心得）

[![img](.assets/Untitled/2-9636b13945b9ccf345bc98d0d81074eb.jpg)](https://www.jianshu.com/u/a417f95b0f53)

[wangrui927](https://www.jianshu.com/u/a417f95b0f53)关注

0.0652018.04.11 09:12:49字数 2,046阅读 3,874

从去年九月底到现在，使用influxdb也有半年时间了。对于监控、用户行为等数据，选择该数据库绝对是没有大方向上的问题的。但是也有一些地方需要注意。
比如：

# 需大致预估数据量，如果不是商业版，需要在单点瓶颈到来前做负载分流。

influxdb具有很强地并发写入能力，我没有做过具体的测试，但根据与其他使用者的沟通交流得知，一般主流配置下，每秒数万次的写入请求是非常轻松的。因为influxdb的机制，如此并发写入能力需要足够容量与速度的内存支持。更重要的一点，可以理解在influxdb中维护了许多时间轴，而数据库名、存储策略、measurement（类似mysql的表）名与tag名一起作为时间轴的标记（series）。也就是说，假设你把一个用户的数据复制并存储了两份，存在相同的数据库中，存在相同的表中，只不过第一份数据的保存策略是29天，第二份数据的保存策略是30天。那么也会被当作两份series来维护。而series的数目，是有上限的。

# 在使用之前，需要根据数据源的用途构建存储策略

跟着上部分，我最早在influxdb中存储的数据是用户上报的数据。这些数据和服务器日志不同，除了能获取“做了什么动作”之外的信息，还能获取到详细的具体数值。数据类别也不少，有用户的地理位置，有多方面的网络状况信息，有用户的对话状况。
我将这些数据拆分并初步加工后（最主要就是清洗数据与时间戳字符串转化为时间段信息），将他们分成了八九个measurement——甚至将会话信息都根据用户在其中的角色分成了讲话者与收听者。然后，我便可以通过数据库本身的聚合函数快速地得到一些数据，比如某个用户在某段时间的收听或发言的总时长。还可以查看这个用户某段时间的信号值状况，来侧面判断是否是运营商网络问题给用户带来延迟等不良体验。
除了可以查询这些与业务有关的数据外，我还可以检测上报数据中与服务监控有关的项目：比如通话震颤与掉线次数。而后者也确实立了功，由于这些数据，物联网提供商再也无法抵赖了，因为我们能根据上报的数据提供出精确的网络波动时间（用户掉线次数激增）。
但好景不长，在这样使用3个月之后，数据库似乎就到达了瓶颈，特别当前端页面发送较复杂的查询请求时，甚至一两分钟都无法得到结果。一方面的原因是安装数据库的机器配置很低，只有两核心，4gb内存。另外一点就是虽然只有大约10万用户上报过数据，但由于我创建了太多measurement，实际上数据库维护了80万个series。而上文提到，series是在内存中维护的。
另外在一次数据迁移中，还出现了类似bug的情况，series数目直接翻倍（这些信息可以通过influxdb内建的_internal数据库查看到），然后数据库支撑没有若干秒就关闭了。我还把这个问题上报给influxdb github的issue中。但最后也没了消息。（当时的版本为influxdb1.3.0）
我也是最近才意识到，就像根据数据的不同需求将数据存储到不同区域的道理一样，在使用influxdb之前也应该仔细设想一下使用情景来设计一下存储策略。比如我现在遇到的问题，我就应该明确，该库的主要作用是监控。所以应该筛选出与监控有关的参数，比如用户上报日志中的tcp udp的延迟信息、丢包数目等。这样series数目差不多就能减少一半。
其次，虽然说influxdb的数据占用空间确实不多，但如果一条series前30天有数据，后30天没有数据，它也仍然会被维护在内存中。而对于运维的监控系统而言，我们更关心的是最近与当下的状况，所以retention policy，也就是influxdb的保存策略，应该是一个较短的时间，比如7天或者15天。由此避免未更新数据的series占据监控系统的资源。

# 之前的用途怎么办？

当然，必须和监控所用的数据库或者环境隔离开。之后，我们仍需要注意一点，就是在存入数据的时候，应该由上报程序做第一层“总结”。比如每隔5分钟上报数据，因为我们想了解用户的情况，往往在意的是用户某一段时间的总体状况，而非他某时某分某秒讲话情况如何。还有就是数据处理的方式，也可以从stream流任务更改成batch批任务。这两者的区别可以在我之前的一篇文章中了解到。
因为我根据对若干天业务服务器的日志分析发现，夜晚服务器的事务处理压力大约只有白天的四分之一。可以利用这段时间去处理实效性要求并不高的业务数据。

# v2版本很适合做临时的检测方案

v2版本主要有这么几个特性，个人已经“以身试法了”，这里简单分享几点个人心得：
1，一个可执行文件
chronograf kapacitor influxdb都整合到一起了，一个二进制可执行文件执行后，监听9999端口，这个端口既是web页面的端口，也是数据库的监听端口。
2，权限全面增强，新增了token，可以使用token进行数据读写操作
3，DSL又改了！这一点有点让人心烦，之前为了使用kapacitor学习的tick配置文件，现在变成flux QL，这样又增加了不少学习成本。
不过这次fluxQL的目的是为了通过这一个DSL来解决定时、流任务，数据查询等多种操作，并且相比tick，能执行的操作更多。
4，接口返回数据的格式改为csv，并且可以使用fluxQL来自定义返回数据的格式与字段
5，增加了类似prometheus的scrape功能，不过似乎默认是10秒的采集间隔，这样可以直接采集prometheus exporter上的数据，如果你使用1.7或更低版本，可能需要在prometheus中采集数据时使用remote_write的功能，将数据点写入influxdb。
现在influxdb v2直接可以实现这个采集的操作
6，流任务更加直观。现在你可以查询数据时，把查询操作直接保存成定时任务，然后将生成的数据backfill进influxdb中。
并且这个定时任务的管理器功能更加强大，你不仅可以设置定时执行，还能立刻执行





# InfluxDB频繁OOM问题排查

[![img](.assets/Untitled/96.jpeg)](https://www.jianshu.com/u/9c46ece5b7bd)

[BGbiao](https://www.jianshu.com/u/9c46ece5b7bd)关注

0.4322019.07.28 22:04:44字数 2,708阅读 2,841

> 记录一次Influxdb频繁OOM问题排查过程。

### 问题现象

业务方发现写入Influxdb超时，实例[http://influxdb:8086/](https://links.jianshu.com/go?to=http%3A%2F%2Finfluxdb%3A8086%2F) 接口调用超时。

### 问题查看

登陆influxdb查看进程状态，发现influxdb实例存活，查看influxdb实例启动时间为`业务超时时间`前后。通过简单查看influxdb服务发现该服务使用systemd进行服务管理，并且配置了`on-failer=restart`策略，导致进程挂掉后会自动重启实例。

`demsg`查看后，发现触发了系统的OOMKiller，导致Influxdb进程被kill掉了。



```csharp
[4312996.555762] Out of memory: Kill process 30539 (influxd) score 979 or sacrifice child
[4312996.557382] Killed process 30539 (influxd) total-vm:86617144kB, anon-rss:64314244kB, file-rss:0kB, shmem-rss:0kB
[4314332.278302] java invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0
```

**思考:** 其实任何服务配置了自动重启策略在大多数场景都会比较有隐患，因为如果业务侧的敏感度较低，该策略会在异常时自动重启，对于业务影响就是重启的节点服务不稳定，但是从长远角度来讲，其实是埋下了一个祸根，因为重启永远不能解决根本问题。而且随着时间的推移，风险会一直增长。

### 问题排查

**1. 添加InfluxDB进程监控**

由于influxdb缺少一些基础进程和指标监控，导致每次重启后都是由业务方反馈出来的，因此刚开始很难知道那个时间点进行了重启，并且无法很直观去对比重启时间点前后的任务并进行分析。所以排查的第一步就是先加个进程监控，去及时发现进程在什么时间点OOM了，然后对照基础监控，对比前后时间点的异常问题进行分析.



```bash
$ crontab  -l
*/10 * * * * /bin/bash /root/bgbiao/check_influxdb_start_time.sh influxdb

$ cat /root/xxb/check_influxdb_start_time.sh
#!/bin/bash
process=$1
nowday=`date +%Y%m%d`
pid=$(ps -ef | grep $process | grep "/usr/bin/" | grep -v grep | awk '{print $2}')
echo $pid
starttime=`ps -eo pid,lstart,etime | grep ${pid}`
echo ${starttime} >> /root/bgbiao/${nowday}.log

$ cat 20190722.log | awk '{print $1 " "$5}' | sort -u -k 2
16527 11:19:36
26477 14:18:55
843 16:21:22
7691 18:17:01
10415 19:03:11
11462 19:27:25
13892 20:09:09
14439 20:17:41
29097 21:17:27
27207 23:18:52
$ cat 20190723.log | awk '{print $1 " "$5}' | sort -u -k 2
30586 00:17:46
1582 01:17:37
5106 02:17:22
8585 03:20:50
11075 04:03:09
11896 04:17:33
18683 06:17:07
25491 08:17:10
28991 09:21:21
6090 10:22:29
10870 11:17:49
25714 12:17:44
28253 13:00:31
24379 14:17:26
27917 15:19:33
31417 16:17:46
2645 17:17:35
6256 18:19:25
9590 19:17:37
13166 20:17:40
15018 20:48:46
16799 21:17:53
23597 23:17:29
27207 23:18:52
$ cat 20190724.log | awk '{print $1 " "$5}' | sort -u -k 2
27057 00:18:34
30548 01:25:02
1173 02:18:29
4654 03:17:43
8358 04:17:57
11801 05:17:28
15244 06:17:37
18690 07:17:32
22158 08:18:06
25575 09:17:43
29220 10:17:52
32748 11:17:43
4199 12:18:55
7613 13:17:42
11090 14:17:31
15237 15:17:34
18792 16:17:44
22363 17:17:28
25808 18:17:48
29287 19:17:38
314 20:17:44
3926 21:17:34
7450 22:18:01
23597 23:17:29
10914 23:17:36
```

从前两天的监控来看，发现该influxdb的OOM问题非常严重，平均一小时OOM一次，然后实例自动重启，由于Influxdb在重启过程中需要加载磁盘中的数据，过程中会有短暂的几分钟业务不可用，此时业务方就会立马反馈出来，但是由于实例很快就重启，非敏感型的业务通常也关注不到，或者不关注该问题。

发现OOM和重启时间点后就和主机的基础监控进行对比，其实发现只有内存在OOM前会大量的增加(基本上突然达到90%后会一直上涨，而后就是OOM，继而systemd重启实例，奇怪的是阿里云也有Influxdb的进程级别的内存监控，进程的内存却一直很平稳)

**2. InfluxDB问题追踪**

既然发现了OOM的时间点，也对比了监控，除了内存，其他也没什么性能瓶颈，那就需要稍微深入Influxdb进行问题追踪了。

> 由于之前最早在搞Docker的时候用过[TIG](https://www.jianshu.com/p/378d0005c0a4)
> (Telegraf+Influxdb+Grafana)方案，对Influxdb有一定了解，知道如果在并发查询很大的情况下磁盘会是瓶颈(毕竟是在磁盘上直接读写数据)，并且单库的序列(series)不能太多，但是当时由于主要监控基础资源的监控指标，数据量和组合序列也并不是很多，并没有太细致的关注。

深入[Influxdb官网](https://links.jianshu.com/go?to=https%3A%2F%2Fdocs.influxdata.com%2Finfluxdb%2Fv1.7%2Ftroubleshooting%2Ffrequently-asked-questions%2F%23why-does-series-cardinality-matter)发现如下描述:

> InfluxDB maintains an in-memory index of every series in the system. As the number of unique series grows, so does the RAM usage. High series cardinality can lead to the operating system killing the InfluxDB process with an out of memory (OOM) exception. See SHOW CARDINALITY to learn about the InfluxSQL commands for series cardinality.

大致意思就是说，InfluxDB会在系统上为每个`series`维护一个`内存索引`，而随着这些series的增加，RAM内存使用率也会增加。如果[series cardinality](https://links.jianshu.com/go?to=https%3A%2F%2Fdocs.influxdata.com%2Finfluxdb%2Fv1.7%2Fconcepts%2Fglossary%2F%23series-cardinality)如果太高，就会导致操作系统触发OOMKiller机制，将Influxdb进程KILL掉. 使用[SHOW CARDINALITY](https://links.jianshu.com/go?to=https%3A%2F%2Fdocs.influxdata.com%2Finfluxdb%2Fv1.7%2Fquery_language%2Fspec%2F%23show-cardinality)命令可以查看到`series cardinality`。

```
注意: 一般情况下，一定要查阅当前版本匹配的文档
```

好了，看到如上官方说明，先去看下influxdb上各个库的series统计:



```ruby
# 查看目标库的series cardinality
$ influx -execute "SHOW SERIES CARDINALITY on kafka_monitor"
cardinality estimation
----------------------
833
$ influx -execute "SHOW SERIES CARDINALITY on recommend"
cardinality estimation
----------------------
5853490
$ influx -execute "SHOW SERIES CARDINALITY on recommend_day"
cardinality estimation
----------------------
15848
$ influx -execute "SHOW SERIES CARDINALITY on rtbusimon"
cardinality estimation
----------------------
5000128
$ influx -execute "SHOW SERIES CARDINALITY on rtmysql"
cardinality estimation
----------------------
881925

$ influx -execute "SHOW SERIES CARDINALITY on datamon"
cardinality estimation
----------------------
114
```

从上面的基础排查中，可以发现`rtbusimon`和`recommend`两个库的series cardinality太多，也就是很可能是它俩造成的OOM，和业务方沟通过后发现`rtbusimon`库的数据已经不再使用，因此基本可以断定是`recommend`库导致的。

基本发现问题之后，很快就有验证猜测和解决方案。要么让业务库优化schema,要么优化influxdb(限制内存或者优化配置)，很明显优化schema是一个长期的问题，而且该influxdb上运行了其他业务库，为了验证是`recommend`造成的OOM，并且为了隔离影响，决定将`recommend`库迁移出去，另外使用内存限制和其他influxdb优化策略来尝试解决。

在迁移`recommend`之前，其实也做了很多配置优化，比如前面官方说到了，系统会为每个`series`维护一个内存索引，也就是说这些`series太多，数据量较大，且又在内存里，因而导致内存使用率不断上升`，因此其实可以在这个索引上去寻找优化方案。

在InfluxDB官方配置文档中发现一个索引参数`index_version`，用来指定索引类型，默认是`inmem`，也就是将上面这些series信息的索引维护在内存里，同事还支持`tsi1`类型，该中类型是后来版本中支持的一种索引类型，主要优化是将索引数据存储在磁盘上，并使用冷热数据分离的方式让整个库可以支持更多的series。(幸亏我们使用的是Influxdb1.X的最高版本1.7，是支持tsi1索引类型的)

[influxdb-tsi索引细节](https://links.jianshu.com/go?to=https%3A%2F%2Fdocs.influxdata.com%2Finfluxdb%2Fv1.7%2Fconcepts%2Ftsi-details%2F)

在influxdb老库上修改`index-version = "tsi1"`，等待下次OOM后重启实例生效，但是观察了几个小时后发现依然会不断OOM，好像`tsi1`索引类型不生效，查看相关日志看到如下日志:



```ruby
Jul 24 04:18:17 izbp169jm0y04a93txacaqz influxd: ts=2019-07-23T20:18:17.278376Z lvl=info msg="Opened shard" log_id=0Gp7_f5l000 service=store trace_id=0Gp7_fVW000 op_name=tsdb_open index_version=inmem path=/var/lib/influxdb/data/kafka_monitor/kafka_mointor_rp/1187 duration=97.488ms
Jul 24 04:18:17 izbp169jm0y04a93txacaqz influxd: ts=2019-07-23T20:18:17.280005Z lvl=info msg="Opened shard" log_id=0Gp7_f5l000 service=store trace_id=0Gp7_fVW000 op_name=tsdb_open index_version=inmem path=/var/lib/influxdb/data/kafka_monitor/kafka_mointor_rp/1192 duration=187.117ms
```

果真，虽然已经改为`index_version="tsi1"`了，但是貌似数据依然使用的是`inmem`类型，导致series数据依然使用内存索引，influxdb依然是每小时OOM重启。但是，不应该啊，毕竟官网说了`tsi1`是磁盘索引，可是为啥不生效呢

继续排查相关资料，发现官网的一片博客[Path to 1 Billion Time Series: InfluxDB High Cardinality Indexing Ready for Testing](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.influxdata.com%2Fblog%2Fpath-1-billion-time-series-influxdb-high-cardinality-indexing-ready-testing%2F)，基本上也是他们对series-cardinality较高场景的一些测试，其中也提到了使用`tsi1`索引来满足这种高基序列(high-series-cardinality)的处理，同时需要注意的是，修改完`tsi1`索引后首先要重启才能生效，并且如果influxdb已经有一些数据，那么老分片上的数据将会一直使用`inmem`索引类型，而只有新分片上的数据才会使用`tsi1`的磁盘索引方式。

那么如何区分哪些分片是使用`inmem`还是`tsi1`索引类型呢，可以查看分片目录，如果有`index`目录，即表示该分片使用`tsi1`的磁盘索引。



```kotlin
# 后面那个数字其实就是分片id，通常id都是递增的，因此一般比较大的分片id应该就是新创建的分片
$ ls /var/lib/influxdb/data/recommend/one_month_only/1023
000000010-000000002.tsm  fields.idx

$ ls /var/lib/influxdb/data/recommend/one_month_only/1504
000000017-000000002.tsm  fields.idx  index
```

基本上确认`tsi1`索引不生效就是因为仍然有很多数据落在老的分片上。确定了该问题后，为快速验证是`recommend`导致的OOM，且避免影响其他业务库，当即作出迁移`recommend`到其他主机上。

迁移完成后，老库监控的Influxdb(2019-07-25 17:30 正式将recommend库切换到新库运行)进程启动时间如下:



```dart
[root@izbp169jm0y04a93txacaqz xxb]# cat 20190725.log | awk '{print $1 " "$5}' | sort -u -k 2
14373 00:17:35
17838 01:17:36
21287 02:17:43
24738 03:18:56
28149 04:17:31
31654 05:17:28
2661 06:17:28
6239 07:17:25
9714 08:17:49
13160 09:17:31
16624 10:17:46
20105 11:17:43
23567 12:17:40
27035 13:17:42
30539 14:17:50
306 14:55:21
1634 15:17:37
5204 16:17:26
8695 17:17:29
10914 23:17:36
[root@izbp169jm0y04a93txacaqz xxb]# cat 20190726.log | awk '{print $1 " "$5}' | sort -u -k 2
8695 17:17:29
[root@izbp169jm0y04a93txacaqz xxb]# cat 20190727.log | awk '{print $1 " "$5}' | sort -u -k 2
8695 17:17:29
```

可以看到，将`recommend`库从老库迁移出去后，老库在`2019-07-25`号最后一次启动进程信息是: `8695 17:17:29`，且在后续的两天`20190726和20190727`两天内，该influxdb实例没有再出现过OOM，进程重启问题没有再出现.

好了，到现在为止，已经确认了`series cardinility`太高的确会导致OOM，且`inmem`的方式会随着`series`的增加，不断占用系统内存。优化的方式就是尽量不要设计太多`series`，同时推荐使用`tsi`索引类型+SSD方式来权衡.

1. **解决方案**

验证到了问题，并且找到修复问题的方法，接下来就是如何不影响业务的平滑迁移。

其实刚开始迁移的时候，和业务方的沟通的结果是不迁移数据，只是在新库创建库以及库的数据保留策略和聚合策略，由业务方进行双写，也就是既写老库，也写新库，写新库的目的是为了重新生成分片，使`tsi1`的索引类型生效，等数据量满足业务需求时，再进行平滑的库切换。但由于老Influxdb实例有其他业务的库也在用，为了尽快剥离`recommend`库对其他业务的影响，我们将`recommend`库数据也一并迁移过去了，不过迁移完成之后，噩耗才刚刚开始。因为数据迁移是使用`influxd backup和restore`完成的，此时会将数据的分片信息也一并迁移过去，结果是就是虽然新库有了老数据，但是老数据的分片信息也是旧的，依然使用的是`inmem`的索引类型。结果就是，该新库依然频繁OOM。

进程监控信息如下:



```dart
$ cat 20190725.log | awk '{print $1 " "$5}' | sort -u -k 2
20465 08:04:59
18065 16:33:01
22063 18:16:20
22311 19:17:27
22556 20:19:15
22749 21:17:44
22965 22:17:23
23120 23:01:50
23179 23:19:56
$ cat 20190726.log | awk '{print $1 " "$5}' | sort -u -k 2
23331 00:01:58
23379 00:16:33
23526 01:01:07
23584 01:19:33
23734 02:02:35
23792 02:16:29
23946 03:05:05
23995 03:16:39
24186 04:19:11
24448 06:01:16
24499 06:16:44
24683 07:16:41
24884 08:18:35
25071 09:19:21
25272 10:19:23
25472 11:19:16
25547 11:38:38
25719 12:16:24
25889 13:19:45
25961 13:38:37
26099 14:16:20
26438 16:16:49
26613 17:16:32
26796 18:16:28
26970 19:16:25
27163 20:20:08
27382 21:14:53
27536 22:01:13
27592 22:19:28
23179 23:19:56
27764 23:28:06

$ cat 20190727.log | awk '{print $1 " "$5}' | sort -u -k 2
27912 00:02:16
27963 00:16:54
28129 01:03:31
28188 01:18:26
28350 02:15:26
28507 03:03:00
28568 03:21:35
28737 04:09:33
28783 04:16:31
28925 05:02:26
28975 05:16:30
29117 06:01:08
29169 06:19:12
29317 07:02:00
29400 07:20:01
29521 08:01:04
29584 08:16:30
29760 09:17:24
29951 10:19:26
30134 11:20:26
30305 12:20:11
30534 13:58:39
30643 14:20:08
30787 15:19:10
27764 23:28:06
```

好吧，依然是平均一个多小时重启一次。查看索引类型如下:



```swift
$ cat /var/log/messages  | grep -oP 'index_version=(inmem|tsi1)' | sort | uniq -c
   6631 index_version=inmem
    512 index_version=tsi1
```

发现`tsi1`索引类型的数据几乎只占了全部数据的不到10%，剩余的数据依然使用`inmem`的方式来存储每个`series`信息。和业务方沟通后的另外一个方案就是，重新创建一个库，由业务方使用接口重新对历史数据进行补充写入(相当于是使用tsi索引类型重新往一个新库进行全新数据写入)。接下来就要靠业务方将全部数据写入新的`tsi`索引类型的库后继续观察了。

后续有进展的话，再继续补充吧。

