[toc]

## 下载kubernetes服务端

下载 kubernetes 二进制版本包需要科学上网工具

- 进入kubernetes的github页面: https://github.com/kubernetes/kubernetes
- 进入tags页签: https://github.com/kubernetes/kubernetes/tags
- 选择要下载的版本: https://github.com/kubernetes/kubernetes/releases/tag/v1.15.2
- 点击 CHANGELOG-${version}.md  进入说明页面: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.15.md#downloads-for-v1152
- 下载Server Binaries: https://dl.k8s.io/v1.15.2/kubernetes-server-linux-amd64.tar.gz

### 解压安装

```sh
tar -zxf kubernetes-server-linux-amd64.1.19.7.tar.gz
mkdir -p /opt/kubernetes/packages
mv kubernetes /opt/kubernetes/packages/kubernetes.1.19.7
ln -snf /opt/kubernetes/packages/kubernetes.1.19.7 /opt/kubernetes/cur
```

## 部署 kube-apiserver - `主控节点`

### 签发证书

#### 签发 etcd client 证书（apiserver和etcd通信证书）

```sh
cd /opt/certs/k8s/
vim etcd-client-csr.json
```

```json
{
    "CN": "k8s-node",
    "hosts": [
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ]
}
```

```sh
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client etcd-client-csr.json | cfssl-json -bare etcd-client
```

#### 签发 apiserver 证书（apiserver和其它k8s组件通信使用）

```sh
cd /opt/certs/k8s/
vim apiserver-csr.json
```

```json
{
    "CN": "k8s-apiserver",
    "hosts": [
        "127.0.0.1",
        "192.168.0.1",
        "kubernetes.default",
        "kubernetes.default.svc",
        "kubernetes.default.svc.cluster",
        "kubernetes.default.svc.cluster.local",
        "172.16.103.20",
        "172.16.103.200"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ]
}
```

>hosts中将所有可能作为apiserver的ip添加进去，VIP 172.16.103.20 (下面会配置)也要加入

```sh
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server apiserver-csr.json | cfssl-json -bare apiserver
```

### 证书下发

```sh
for i in 200;do echo lc103-$i;ssh lc103-$i "mkdir -p /opt/kubernetes/certs";scp apiserver-key.pem apiserver.pem ca-key.pem ca.pem etcd-client-key.pem etcd-client.pem lc103-$i:/opt/kubernetes/certs/;done
```

### 为 apiserver 创建日志审计配置

> 涉及节点：所有运算节点

```sh
cd /opt/kubernetes
mkdir conf
vim conf/audit.yaml
```

```yaml
apiVersion: audit.k8s.io/v1beta1 # This is required.
kind: Policy
# Don't generate audit events for all requests in RequestReceived stage.
omitStages:
  - "RequestReceived"
rules:
  # Log pod changes at RequestResponse level
  - level: RequestResponse
    resources:
    - group: ""
      # Resource "pods" doesn't match requests to any subresource of pods,
      # which is consistent with the RBAC policy.
      resources: ["pods"]
  # Log "pods/log", "pods/status" at Metadata level
  - level: Metadata
    resources:
    - group: ""
      resources: ["pods/log", "pods/status"]

  # Don't log requests to a configmap called "controller-leader"
  - level: None
    resources:
    - group: ""
      resources: ["configmaps"]
      resourceNames: ["controller-leader"]

  # Don't log watch requests by the "system:kube-proxy" on endpoints or services
  - level: None
    users: ["system:kube-proxy"]
    verbs: ["watch"]
    resources:
    - group: "" # core API group
      resources: ["endpoints", "services"]

  # Don't log authenticated requests to certain non-resource URL paths.
  - level: None
    userGroups: ["system:authenticated"]
    nonResourceURLs:
    - "/api*" # Wildcard matching.
    - "/version"

  # Log the request body of configmap changes in kube-system.
  - level: Request
    resources:
    - group: "" # core API group
      resources: ["configmaps"]
    # This rule only applies to resources in the "kube-system" namespace.
    # The empty string "" can be used to select non-namespaced resources.
    namespaces: ["kube-system"]

  # Log configmap and secret changes in all other namespaces at the Metadata level.
  - level: Metadata
    resources:
    - group: "" # core API group
      resources: ["secrets", "configmaps"]

  # Log all other resources in core and extensions at the Request level.
  - level: Request
    resources:
    - group: "" # core API group
    - group: "extensions" # Version of group should NOT be included.

  # A catch-all rule to log all other requests at the Metadata level.
  - level: Metadata
    # Long-running requests like watches that fall under this rule will not
    # generate an audit event in RequestReceived.
    omitStages:
      - "RequestReceived"
```

### 创建启动脚本

```sh
vim /opt/kubernetes/kube-apiserver-startup.sh
```

```sh
#!/bin/bash

WORK_DIR=$(dirname $(readlink -f $0))
[ $? -eq 0 ] && cd $WORK_DIR || exit

/opt/kubernetes/cur/server/bin/kube-apiserver \
    --apiserver-count 2 \
    --audit-log-path /data/logs/kubernetes/kube-apiserver/audit-log \
    --audit-policy-file ./conf/audit.yaml \
    --authorization-mode RBAC \
    --client-ca-file ./certs/ca.pem \
    --requestheader-client-ca-file ./certs/ca.pem \
    --enable-admission-plugins NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota \
    --etcd-cafile   ./certs/ca.pem \
    --etcd-certfile ./certs/etcd-client.pem \
    --etcd-keyfile  ./certs/etcd-client-key.pem \
    --etcd-servers https://172.16.103.200:23791,https://172.16.103.200:23792,https://172.16.103.200:23793 \
    --service-account-key-file ./certs/ca-key.pem \
    --service-cluster-ip-range 192.168.0.0/16 \
    --service-node-port-range 3000-29999 \
    --target-ram-mb=1024 \
    --kubelet-client-certificate ./certs/etcd-client.pem \
    --kubelet-client-key         ./certs/etcd-client-key.pem \
    --tls-cert-file              ./certs/apiserver.pem \
    --tls-private-key-file       ./certs/apiserver-key.pem \
    --log-dir  /data/logs/kubernetes/kube-apiserver \
    --v 2
```

```sh
mkdir -p /data/logs/kubernetes/kube-apiserver
```

### 使用supervisor启动

```sh
vim /etc/supervisord.d/kube-apiserver.ini
```

```ini
[program:kube-apiserver-103-200]
command=/opt/kubernetes/kube-apiserver-startup.sh
numprocs=1
directory=/opt/kubernetes/
autostart=true
autorestart=true
startsecs=30
startretries=3
exitcodes=0,2
stopsignal=QUIT
stopwaitsecs=10
user=root
redirect_stderr=true
stdout_logfile=/data/logs/kubernetes/kube-apiserver/apiserver.stdout.log
stdout_logfile_maxbytes=64MB
stdout_logfile_backups=5
stdout_capture_maxbytes=1MB
stdout_events_enabled=false
```

```sg
mkdir -p /data/logs/kubernetes/kube-apiserver
supervisorctl update
supervisorctl status
```

```sh
supervisorctl start kube-apiserver-103-200
supervisorctl stop kube-apiserver-103-200
supervisorctl restart kube-apiserver-103-200
supervisorctl status kube-apiserver-103-200
```

查看进程

```sh
netstat -lntp | grep api
```

## 配置apiserver L4代理 - `主控节点`

> 所有主控节点

```sh
yum install -y nginx

vim /etc/nginx/nginx.conf
```

末尾添加

```conf
# 末尾加上以下内容，stream 只能加在 main 中
# 此处只是简单配置下nginx，实际生产中，建议进行更合理的配置

stream {
    log_format proxy '$time_local|$remote_addr|$upstream_addr|$protocol|$status|'
                     '$session_time|$upstream_connect_time|$bytes_sent|$bytes_received|'
                     '$upstream_bytes_sent|$upstream_bytes_received' ;

    upstream kube-apiserver {
        server 172.16.103.200:6443     max_fails=3 fail_timeout=30s;
    }
    server {
        listen 7443;
        proxy_connect_timeout 2s;
        proxy_timeout 900s;
        proxy_pass kube-apiserver;
        access_log /var/log/nginx/proxy.log proxy;
    }
}
```

> 这里用  7443 端口 反向代理 所有 运算节点 apiserver的 6443 端口，所以所有的 apiserver 节点，应统一添加到 upstream



```sh
nginx -t                           # 检查配置
systemctl restart nginx            # 重启 nginx
```

### 配置高可用

> * 对于多个主控服务节点，需要配置高可用，这里假定还有一台节点 172.16.103.201
> * 一个节点也可以配置，当然在实际中就不具有高可用性

```sh
# 所有主控节点 
yum install keepalived -y
```

```sh
vim /etc/keepalived/check_port.sh
```

```sh
#!/bin/bash

CHK_PORT=$1
if [ -n "$CHK_PORT" ];then
    PORT_PROCESS=`ss -lnt | grep $CHK_PORT | wc -l`
    if [ $PORT_PROCESS -eq 0 ];then
        echo "Port $CHK_PORT Is Not Used,End."
        exit 1
    fi
else
    echo "Check Port Cant Be Empty!"
fi
```

此脚本用来检查端口是否存活，使用时用 参数1 传入端口号，下面添加可执行权限

```sh
chmod +x /etc/keepalived/check_port.sh
```

#### 主节点

```sh
vim /etc/keepalived/keepalived.conf 
```

```ini
! Configuration File for keepalived

global_defs {
    router_id 172.16.103.200
}

vrrp_script chk_nginx {
    script "/etc/keepalived/check_port.sh 7443"
    interval 2
    weight -20
}

vrrp_instance VI_1 {
    state MASTER                    # 角色是master
    interface eth0                  # vip 绑定端口
    virtual_router_id 251           # 让 master 和 backup 在同一个虚拟路由里，id 号必须相同；
    priority 100                    # 优先级,谁的优先级高谁就是master ;
    advert_int 1
    mcast_src_ip 172.16.103.200
    nopreempt                       # 重要：非抢占式
    authentication {
        auth_type PASS
        auth_pass 11111111
    }
    track_script {
        chk_nginx
    }
    virtual_ipaddress {
        172.16.103.20               # 虚拟ip
    }
}
```

#### 从节点

```sh
vi /etc/keepalived/keepalived.conf 
```

```ini
! Configuration File for keepalived

global_defs {
    router_id 172.16.103.201
    script_user root
    enable_script_security
}

vrrp_script chk_nginx {
    script "/etc/keepalived/check_port.sh 7443"
		interval 2
		weight -20
}

vrrp_instance VI_1 {
		state BACKUP
		interface ens33		
		virtual_router_id 251
		mcast_src_ip 172.16.103.201
		priority 90
		advert_int 1
		authentication {
				auth_type PASS
				auth_pass 11111111
		}
		track_script {
				chk_nginx
		}
		virtual_ipaddress {
				172.16.103.20
		}
}
```

> 上面注意修改 interface 对应的网卡设备名

这里主从节点均设置虚拟 ip 为 172.16.103.20，那么就可以由 这个ip 访问到两台机器，生效的机器会根据脚本的检测结果自动切换

```sh
# 两台节点均需部署
systemctl enable --now keepalived

# 查看 master 节点 的 eth0 是否绑定设定的 ip
ip add
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:0c:29:89:0e:59 brd ff:ff:ff:ff:ff:ff
    inet 172.16.103.200/24 brd 172.16.103.255 scope global noprefixroute ens33
       valid_lft forever preferred_lft forever
    inet 172.16.103.20/32 scope global ens33
       valid_lft forever preferred_lft forever
    inet6 fe80::20c:29ff:fe89:e59/64 scope link
       valid_lft forever preferred_lft forever

# 在 master 节点上 重启 keepalived
systemctl restart keepalived

# 在 从 节点上 检查 eth0 是否绑定设定的 ip
ip add

# 在 从节点上 重启 keepalived
systemctl restart keepalived
```

## 部署 kube-controller-manager - `主控节点`

kube-controller-manager 设置为只调用当前机器的 apiserver，走127.0.0.1网卡，因此不配制SSL证书

```sh
vim /opt/kubernetes/kube-controller-manager-startup.sh
```

```sh
#!/bin/sh
WORK_DIR=$(dirname $(readlink -f $0))
[ $? -eq 0 ] && cd $WORK_DIR || exit

/opt/kubernetes/cur/server/bin/kube-controller-manager \
    --cluster-cidr 172.7.0.0/16 \
    --leader-elect=true \
    --log-dir /data/logs/kubernetes/kube-controller-manager \
    --master http://127.0.0.1:8080 \
    --service-account-private-key-file ./certs/ca-key.pem \
    --service-cluster-ip-range 192.168.0.0/16 \
    --root-ca-file ./certs/ca.pem \
    --v 2
```

> * `--cluster-cidr` 设置了pod 网段，后面在部署 kube-proxy 和 flannel 插件时还会用到
> * `--service-cluster-ip-range` 设置了 service 网段

```sh
chmod u+x /opt/kubernetes/kube-controller-manager-startup.sh
```

### 使用 supervisor 启动

```sh
vim /etc/supervisord.d/kube-controller-manager.ini
```

```ini
[program:kube-controller-manager-103-200]
command=/opt/kubernetes/kube-controller-manager-startup.sh                         ; 
numprocs=1                                                                         ; 
directory=/opt/kubernetes                                                          ; 
autostart=true                                                                     ; 
autorestart=true                                                                   ; 
startsecs=30                                                                       ; 
startretries=3                                                                     ; 
exitcodes=0,2                                                                      ; 
stopsignal=QUIT                                                                    ; 
stopwaitsecs=10                                                                    ; 
user=root                                                                          ; 
redirect_stderr=true                                                               ; 
stdout_logfile=/data/logs/kubernetes/kube-controller-manager/controller.stdout.log ; 
stdout_logfile_maxbytes=64MB                                                       ; 
stdout_logfile_backups=4                                                           ; 
stdout_capture_maxbytes=1MB                                                        ;
stdout_events_enabled=false                                                        ; 
```



```sh
mkdir -p /data/logs/kubernetes/kube-controller-manager
supervisorctl update
supervisorctl status
```

## 部署 kube-scheduler - `主控节点`

kube-scheduler 设置为只调用当前机器的 apiserver，走127.0.0.1网卡，因此不配制SSL证书

```sh
vim /opt/kubernetes/kube-scheduler-startup.sh
```

```sh
#!/bin/sh
WORK_DIR=$(dirname $(readlink -f $0))
[ $? -eq 0 ] && cd $WORK_DIR || exit

/opt/kubernetes/cur/server/bin/kube-scheduler \
    --leader-elect  \
    --log-dir /data/logs/kubernetes/kube-scheduler \
    --master http://127.0.0.1:8080 \
    --v 2
```

```sh
chmod u+x /opt/kubernetes/kube-scheduler-startup.sh
```

### 使用 supervisor 启动

```sh
vim /etc/supervisord.d/kube-scheduler.ini
```

```ini
[program:kube-scheduler-103-200]
command=/opt/kubernetes/kube-scheduler-startup.sh                     
numprocs=1                                                               
directory=/opt/kubernetes                                   
autostart=true                                                           
autorestart=true                                                         
startsecs=30                                                             
startretries=3                                                           
exitcodes=0,2                                                            
stopsignal=QUIT                                                          
stopwaitsecs=10                                                          
user=root                                                                
redirect_stderr=true                                                     
stdout_logfile=/data/logs/kubernetes/kube-scheduler/scheduler.stdout.log 
stdout_logfile_maxbytes=64MB                                             
stdout_logfile_backups=4                                                 
stdout_capture_maxbytes=1MB                                              
stdout_events_enabled=false 
```

```sh
mkdir -p /data/logs/kubernetes/kube-scheduler
supervisorctl update
supervisorctl status
```

### 检查主控节点状态

```sh
ln -s /opt/kubernetes/server/bin/kubectl /usr/local/bin/
kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-1               Healthy   {"health": "true"}
etcd-2               Healthy   {"health": "true"}
etcd-0               Healthy   {"health": "true"}
```

## 部署 kubelet - `运算节点`

### 签发证书

```sh
cd /opt/certs/k8s
vim kubelet-csr.json 
```

> **将所有可能的kubelet机器IP添加到`hosts`中**

```json
{
    "CN": "k8s-kubelet",
    "hosts": [
        "127.0.0.1",
        "172.16.103.20",
        "172.16.103.200",
        "172.16.103.201",
        "172.16.103.202"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ]
}
```

```sh
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server kubelet-csr.json | cfssl-json -bare kubelet

scp kubelet.pem kubelet-key.pem lc103-200:/opt/kubernetes/certs/
```

### 创建kubelet配置

* `set-cluster` - **创建需要连接的集群信息，可以创建多个k8s集群信息**

```sh
kubectl config set-cluster myk8s \
--certificate-authority=/opt/kubernetes/certs/ca.pem \
--embed-certs=true \
--server=https://172.16.103.20:7443 \
--kubeconfig=/opt/kubernetes/conf/kubelet.kubeconfig
```

> * 注意，前面如果配置了高可用，server 的 ip 地址应指定为 设定 虚拟 ip
> * 这一步会在制定位置生成配置文件 kubelet.kubeconfig，后面几步操作会以此配置文件为基础添加配置

* `set-credentials`  # 创建用户账号，即用户登陆使用的客户端私有和证书，可以创建多个证书

```sh
kubectl config set-credentials k8s-node \
--client-certificate=/opt/kubernetes/certs/etcd-client.pem \
--client-key=/opt/kubernetes/certs/etcd-client-key.pem \
--embed-certs=true \
--kubeconfig=/opt/kubernetes/conf/kubelet.kubeconfig
```

* `set-context`  # 设置context，即确定账号和集群对应关系

```sh
kubectl config set-context myk8s-context \
--cluster=myk8s \
--user=k8s-node \
--kubeconfig=/opt/kubernetes/conf/kubelet.kubeconfig
```

> 这里设置了 cluster 和 user，为前面两步配置时使用的名称

* `use-context`  # 设置当前使用哪个context

```sh
kubectl config use-context myk8s-context --kubeconfig=/opt/kubernetes/conf/kubelet.kubeconfig
```

把此配置文件传给另一台就不用做以上四步

```sh
scp /opt/kubernetes/conf/kubelet.kubeconfig lc103-201:/opt/kubernetes/conf/
```

### 授权k8s-node用户

**此步骤只需要在一台master节点执行**

授权 k8s-node 用户绑定集群角色 system:node ，让 k8s-node 成为具备运算节点的权限。

```sh
vim k8s-node.yaml
```

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding  # 资源类型：集群角色绑定
metadata:
  name: k8s-node          # 资源名称
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole       # 集群角色
  name: system:node
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User              # 用户
  name: k8s-node
```

> 以上配置文件 绑定 用户 k8s-node 到 集群角色，并拥有 system:node （运算节点）的权限

```sh
# 提交文件创建资源
kubectl create -f k8s-node.yaml

# 查看资源
kubectl get clusterrolebinding k8s-node
```

### 准备 pause 镜像

```sh
docker image pull kubernetes/pause
docker image tag kubernetes/pause:latest harbor.od.com/public/pause:latest
docker login -u admin harbor.od.com
docker image push harbor.od.com/public/pause:latest
```

### 创建启动脚本

```sh
vim /opt/kubernetes/kubelet-startup.sh
```

```sh
#!/bin/sh

WORK_DIR=$(dirname $(readlink -f $0))
[ $? -eq 0 ] && cd $WORK_DIR || exit

/opt/kubernetes/cur/server/bin/kubelet \
    --anonymous-auth=false \
    --cgroup-driver systemd \
    --cluster-dns 192.168.0.2 \
    --cluster-domain cluster.local \
    --runtime-cgroups=/systemd/system.slice \
    --kubelet-cgroups=/systemd/system.slice \
    --fail-swap-on="false" \
    --client-ca-file ./certs/ca.pem \
    --tls-cert-file ./certs/kubelet.pem \
    --tls-private-key-file ./certs/kubelet-key.pem \
    --hostname-override lc103-200.host.com \
    --image-gc-high-threshold 20 \
    --image-gc-low-threshold 10 \
    --kubeconfig ./conf/kubelet.kubeconfig \
    --log-dir /data/logs/kubernetes/kube-kubelet \
    --pod-infra-container-image harbor.od.com/public/pause:latest \
    --root-dir /data/kubelet
```

> 注意根据实际情况修改如下配置：
>
> * `hostname-override` - 修改为所部署节点的节点域名或ip地址
>
> * `cluster-dns` - 设置dns服务器为 192.168.0.2，后续部署 coredns 服务发现时会使用此 地址
> * `pod-infra-container-image` - 设置为我们创建的私有镜像服务

```sh
chmod u+x /opt/kubernetes/kubelet-startup.sh
```

### 使用 supervisor 启动

```sh
vim /etc/supervisord.d/kube-kubelet.ini
```

```ini
[program:kube-kubelet-103-200]
command=/opt/kubernetes/kubelet-startup.sh
numprocs=1
directory=/opt/kubernetes
autostart=true
autorestart=true
startsecs=30
startretries=3
exitcodes=0,2
stopsignal=QUIT
stopwaitsecs=10
user=root
redirect_stderr=true
stdout_logfile=/data/logs/kubernetes/kube-kubelet/kubelet.stdout.log
stdout_logfile_maxbytes=64MB
stdout_logfile_backups=5
stdout_capture_maxbytes=1MB
stdout_events_enabled=false
```

```sh
mkdir -p /data/logs/kubernetes/kube-kubelet /data/kubelet
supervisorctl update
supervisorctl status
```

### 修改节点角色

使用 kubectl get nodes 获取的Node节点角色为空，可以按照以下方式修改

```sh
 ~]# kubectl get node
NAME                 STATUS   ROLES    AGE   VERSION
lc103-200.host.com   Ready    <none>   17s   v1.19.7
 ~]# kubectl label node lc103-200.host.com node-role.kubernetes.io/node=
node/hdss7-21.host.com labeled
 ~]# kubectl label node lc103-200.host.com node-role.kubernetes.io/master=
node/hdss7-21.host.com labeled
 ~]# kubectl get node
NAME                 STATUS   ROLES         AGE   VERSION
lc103-200.host.com   Ready    master,node   89s   v1.19.7
```

## 部署 kube-proxy

### 签发证书

```sh
cd /opt/certs/k8s
vim kube-proxy-csr.json  # CN 其实是k8s中的角色
```

```json
{
    "CN": "system:kube-proxy",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "beijing",
            "L": "beijing",
            "O": "od",
            "OU": "ops"
        }
    ]
}
```

```sh
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client kube-proxy-csr.json | cfssl-json -bare kube-proxy-client
```

```sh
scp kube-proxy-client-key.pem kube-proxy-client.pem lc103-200:/opt/kubernetes/certs/
```

### 创建 proxy 配置

```sh
kubectl config set-cluster myk8s \
--certificate-authority=/opt/kubernetes/certs/ca.pem \
--embed-certs=true \
--server=https://172.16.103.20:7443 \
--kubeconfig=/opt/kubernetes/conf/kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy \
--client-certificate=/opt/kubernetes/certs/kube-proxy-client.pem \
--client-key=/opt/kubernetes/certs/kube-proxy-client-key.pem \
--embed-certs=true \
--kubeconfig=/opt/kubernetes/conf/kube-proxy.kubeconfig

kubectl config set-context myk8s-context \
--cluster=myk8s \
--user=kube-proxy \
--kubeconfig=/opt/kubernetes/conf/kube-proxy.kubeconfig

kubectl config use-context myk8s-context --kubeconfig=/opt/kubernetes/conf/kube-proxy.kubeconfig
```

> * 第一个命令中 --server=https://172.16.103.20:7443 需要修改为反代的ip地址和端口

```sh
scp kube-proxy.kubeconfig lc103.201:/opt/kubernetes/conf/
```

### 加载ipvs模块

kube-proxy 共有3种流量调度模式，分别是 namespace，iptables，ipvs，其中ipvs性能最好

```sh
for i in $(ls /usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs|grep -o "^[^.]*");do echo $i; /sbin/modinfo -F filename $i >/dev/null 2>&1 && /sbin/modprobe $i;done

lsmod | grep ip_vs  # 查看ipvs模块
```

### 创建启动脚本

```sh
vim /opt/kubernetes/kube-proxy-startup.sh
```

```sh
#!/bin/sh

WORK_DIR=$(dirname $(readlink -f $0))
[ $? -eq 0 ] && cd $WORK_DIR || exit

/opt/kubernetes/cur/server/bin/kube-proxy \
  --cluster-cidr 172.7.0.0/16 \
  --hostname-override lc103-200.host.com \
  --proxy-mode=ipvs \
  --ipvs-scheduler=nq \
  --kubeconfig ./conf/kube-proxy.kubeconfig
```

> * `--cluster-cidr` 这里设置了集群的网段，在后面部署flannel插件时还会用到
> * `--hostname-override` 需要修改为当前主机名

```sh
chmod u+x  /opt/kubernetes/kube-proxy-startup.sh
```

### 使用 supervisor 启动

```sh
vim /etc/supervisord.d/kube-proxy.ini
```

```ini
[program:kube-proxy-103-200]
command=/opt/kubernetes/kube-proxy-startup.sh                
numprocs=1                                                      
directory=/opt/kubernetes                            
autostart=true                                                  
autorestart=true                                                
startsecs=30                                                    
startretries=3                                                  
exitcodes=0,2                                                   
stopsignal=QUIT                                                 
stopwaitsecs=10                                                 
user=root                                                       
redirect_stderr=true                                            
stdout_logfile=/data/logs/kubernetes/kube-proxy/proxy.stdout.log
stdout_logfile_maxbytes=64MB                                    
stdout_logfile_backups=5                                       
stdout_capture_maxbytes=1MB                                     
stdout_events_enabled=false
```

```sh
mkdir -p /data/logs/kubernetes/kube-proxy
supervisorctl update
supervisorctl status
```

### 验证集群

```sh
# supervisorctl status
etcd-server-7-21                 RUNNING   pid 23637, uptime 2 days, 0:27:18
kube-apiserver-7-21              RUNNING   pid 32591, uptime 1 day, 2:06:47
kube-controller-manager-7-21     RUNNING   pid 33357, uptime 1 day, 0:11:02
kube-kubelet-7-21                RUNNING   pid 37232, uptime 9:32:01
kube-proxy-7-21                  RUNNING   pid 47088, uptime 0:06:19
kube-scheduler-7-21              RUNNING   pid 33450, uptime 1 day, 0:01:43
```

```sh
# yum install -y ipvsadm

# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  192.168.0.1:443 nq
  -> 172.16.103.200:6443          Masq    1      0          0
  
# kubectl get svc
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   192.168.0.1   <none>        443/TCP   8d
```

#### 创建一个nginx

```
cd /opt/kubernetes/conf/
vim nginx-ds.yaml
```

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata: 
  name: nginx-ds
spec:
  selector:
    matchLabels:
      app: nginx-ds
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: my-nginx
        image: harbor.od.com/public/nginx:v1.7.9
        ports: 
        - containerPort: 80
```

```sh
kubectl create -f  nginx-ds.yaml

kubectl get pods -owide # 多次运行查看状态
NAME             READY   STATUS    RESTARTS   AGE   IP            NODE                 NOMINATED NODE   READINESS GATES
nginx-ds-pcnk9   1/1     Running   0          85s   172.7.103.2   lc103-200.host.com   <none>           <none>
```

## 新增节点时

1. 签发证书，修改 kubelet-csr.json，添加新增节点的 ip 到
2. 拷贝 kubelet.pem kubelet-key.pem 到新节点对应位置